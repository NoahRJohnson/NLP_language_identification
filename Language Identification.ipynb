{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Identification\n",
    "\n",
    "In order to extract any kind of information from text, the first thing we have to know is what language the text is in. In this assignment you are going to use **character N-gram grammars** to solve the problem of *language identification*.\n",
    "\n",
    "Given a document, your goal is to say what language it is written in. We will give you a set of training documents (one in each of 6 languages) and a set of development test documents. You will be graded on an unseen set of 6 test documents. To make the problem tractable, we guarantee that the test documents will come from one of the 6 languages you have seen in the training set.\n",
    "\n",
    "The data you will use is 6 translations of part of the [Universal Declaration of Human Rights](http://www.un.org/en/universal-declaration-human-rights/index.html) (which has been translated into many languages although the data for the 6 languages is in the Language Identification folder in the Week 04 folder.)\n",
    "\n",
    "The algorithm you will use requires that you build 6 separate character bigram grammars, one for each language, on the training data. Mostly in lecture we talked about **word bigrams**. A character bigram is computed on characters instead of words. You should use the simple Bayesian Unigram Prior smoothing method.\n",
    "\n",
    "For each test document in the dev subfolder, for each of your 6 bigram grammars, you compute the log-likelihood of the test document given the bigram grammar (use the log-likelihood instead of the likelihood since it's less likely to underflow). Then you choose as your answer for that document the language that gave the highest log-likelihood.\n",
    "\n",
    "Here's the formal description of the equations you should be computing. First, you want to pick the language, out of the 6 languages, which assigns the highest log probability to the document:\n",
    "\n",
    "$$\\hat{L} = \\underset{L \\in \\mathcal{L}}{\\mathrm{argmax}} ( log P_L(Document) )$$\n",
    "\n",
    "To compute the log probability for each language, you make the Markov (N-gram) assumption, and use a bigram grammar that has been trained on that language: \n",
    "\n",
    "$$log P_L(Document) = log P_{smooth}(char^n_1) \\approx \\prod_{i=1}^n log P_{smooth}(char_i | char_{i-1})$$\n",
    "\n",
    "(That was the equation in log-space; in non-log space it would be:)\n",
    "\n",
    "$$P_L(Document) = P_{smooth}(char^n_1) \\approx \\prod_{i=1}^n P_{smooth}(char_i | char_{i-1})$$\n",
    "\n",
    "Don't forget to add some sort of special START and END characters at the beginning and end of the file.\n",
    "\n",
    "To train your bigram grammars, use Bayesian Unigram Prior smoothing:\n",
    "\n",
    "$$P_{smooth}(char_i | char_{i-1}) = \\frac{C(char_{i-1},char_i) + P(char_i)}{C(char_{i-1}) + 1}$$\n",
    "\n",
    "Please develop your solution in an iPython notebook using the text in the train subfolder. Then test your models on data in the dev subfolder.\n",
    "\n",
    "The data is in UTF-8 format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training files:  ['esper.txt', 'eng.txt', 'dut.txt', 'frn.txt', 'spn.txt', 'ger.txt']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from collections import Counter\n",
    "\n",
    "import codecs\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "TRAIN_FOLDER = \"train\"  # folder with training files\n",
    "START_CHAR = \"@\"  # unique start character - for bigrams\n",
    "END_CHAR = \"#\"  # unique end character - for bigrams\n",
    "\n",
    "print(\"Training files: \", os.listdir(TRAIN_FOLDER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Read in the text for every training file. Each training file corresponds to a language.\"\"\"\n",
    "train_corpora = {}\n",
    "\n",
    "# read in the training files\n",
    "for train_file in os.listdir(TRAIN_FOLDER):\n",
    "    train_path = os.path.join(TRAIN_FOLDER, train_file)  # full path to training file\n",
    "    language, ext = os.path.splitext(train_file)  # remove extension from file name\n",
    "    try:\n",
    "        corpus_text = codecs.open(train_path, encoding='utf-8').read()  # read unicode\n",
    "        corpus_text = START_CHAR + corpus_text + END_CHAR  # add start and end characters\n",
    "        train_corpora[language] = dict(text=corpus_text)  # store text for this language\n",
    "    except:\n",
    "        print(\"ERROR: Could not read text file \\\"%s\\\"\" % train_file)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"count unigram and bigram frequencies for each language\"\"\"\n",
    "for language in train_corpora.keys():\n",
    "    corpus = train_corpora[language]['text']\n",
    "    \n",
    "    unigram_counts = Counter(corpus)\n",
    "    bigram_counts = Counter(zip(corpus[:-1], corpus[1:]))\n",
    "    \n",
    "    N = sum(unigram_counts.values())  # Number of characters in corpus\n",
    "    V = len(unigram_counts.keys())  # Vocabulary size, number of unique letters in corpus\n",
    "    \n",
    "    unigram_probs = {unigram: count / float(N) \\\n",
    "                     for unigram,count in unigram_counts.iteritems()}  # turn unigram counts into probabilities\n",
    "    \n",
    "    # Store these properties in the dictionary for this language\n",
    "    train_corpora[language]['N'] = N\n",
    "    train_corpora[language]['V'] = V\n",
    "    \n",
    "    train_corpora[language]['unigram_counts'] = unigram_counts\n",
    "    train_corpora[language]['unigram_probs'] = unigram_probs\n",
    "    \n",
    "    train_corpora[language]['bigram_counts'] = bigram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_smooth(a, b, L):\n",
    "    \"\"\"\n",
    "    Returns the probability of seeing character b after character a,\n",
    "    conditioned on language model L.\n",
    "    Uses Bayesian Unigram Prior smoothing for the bigram probability\n",
    "    \"\"\"\n",
    "    global train_corpora\n",
    "    \n",
    "    if L not in train_corpora.keys():\n",
    "        raise ValueError(\"No language model exists for language %s\" % str(L))\n",
    "    \n",
    "    corpus = train_corpora[L]\n",
    "    \n",
    "    N = corpus['N']\n",
    "    unigram_probs = corpus['unigram_probs']\n",
    "    unigram_counts = corpus['unigram_counts']\n",
    "    bigram_counts = corpus['bigram_counts']\n",
    "\n",
    "    p_b = unigram_probs.get(b, 1.0/N)  # unigram probability of b\n",
    "    c_a_b = bigram_counts.get((a,b), 1)  # frequency of bigram (a,b) in this language's training corpus\n",
    "    c_a = unigram_counts.get(a, 1)  # frequency of character a in this language's training corpus\n",
    "    \n",
    "    p_smooth = (c_a_b + p_b) / float(c_a + 1)\n",
    "    \n",
    "    return p_smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['esper.txt', 'eng.txt', 'dut.txt', 'frn.txt', 'spn.txt', 'ger.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_FOLDER = \"dev\"\n",
    "\n",
    "os.listdir(TEST_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_language(document):\n",
    "    \"\"\"Takes in a document string, and classifies the document into one of the languages trained on.\"\"\"\n",
    "    \n",
    "    document = START_CHAR + document + END_CHAR  # add start/end tags\n",
    "    \n",
    "    # Calculate log probabilities for every language L\n",
    "    log_probs = {}\n",
    "    for L in train_corpora.keys():\n",
    "        log_p_L = 0  # log probability of the document belonging to this language\n",
    "        for bigram in zip(test_txt[:-1], test_txt[1:]):\n",
    "            log_p_L += np.log(get_p_smooth(bigram[0], bigram[1], L))  # sum log probs of all bigrams\n",
    "        log_probs[L] = log_p_L\n",
    "    \n",
    "    print(log_probs)\n",
    "    \n",
    "    # get argmax\n",
    "    L_hat = max(log_probs, key=log_probs.get)\n",
    "    \n",
    "    return L_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ger': -5156.617835253816, 'eng': -4830.199488649011, 'esper': -4022.7257458963654, 'frn': -4745.852657185119, 'spn': -4422.47247751263, 'dut': -5401.615711982561}\n",
      "Test file \"esper.txt\" is most likely written in: esper\n",
      "\n",
      "{'ger': -5233.459268912499, 'eng': -4294.395526807493, 'esper': -5001.168904834336, 'frn': -5059.756437228359, 'spn': -5102.926309932263, 'dut': -5020.633862095518}\n",
      "Test file \"eng.txt\" is most likely written in: eng\n",
      "\n",
      "{'ger': -5482.514000130293, 'eng': -5571.293869702195, 'esper': -6126.451056949624, 'frn': -6086.481864218604, 'spn': -6033.183813051615, 'dut': -4899.99404438128}\n",
      "Test file \"dut.txt\" is most likely written in: dut\n",
      "\n",
      "{'ger': -5934.146308425525, 'eng': -5607.85379611641, 'esper': -5623.5734370082055, 'frn': -4932.910949926944, 'spn': -5614.713920954162, 'dut': -5942.935660522131}\n",
      "Test file \"frn.txt\" is most likely written in: frn\n",
      "\n",
      "{'ger': -6008.125468104433, 'eng': -5641.382559918243, 'esper': -5189.878065908862, 'frn': -5553.055824450768, 'spn': -4885.093069296729, 'dut': -6026.309796937171}\n",
      "Test file \"spn.txt\" is most likely written in: spn\n",
      "\n",
      "{'ger': -4926.721579421914, 'eng': -5649.479860529004, 'esper': -5978.888428023162, 'frn': -5946.880456625343, 'spn': -5910.962765010755, 'dut': -5328.161573189936}\n",
      "Test file \"ger.txt\" is most likely written in: ger\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:19: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:21: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "for test_file in os.listdir(TEST_FOLDER):\n",
    "    test_path = os.path.join(TEST_FOLDER, test_file)\n",
    "    \n",
    "    with open(test_path) as f:\n",
    "        test_txt = f.read()\n",
    "    \n",
    "    estimated_language = identify_language(test_txt)\n",
    "    \n",
    "    print(\"Test file \\\"%s\\\" is most likely written in: %s\" % (test_file, estimated_language))\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
