{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Identification\n",
    "\n",
    "In order to extract any kind of information from text, the first thing we have to know is what language the text is in. In this assignment you are going to use **character N-gram grammars** to solve the problem of *language identification*.\n",
    "\n",
    "Given a document, your goal is to say what language it is written in. We will give you a set of training documents (one in each of 6 languages) and a set of development test documents. You will be graded on an unseen set of 6 test documents. To make the problem tractable, we guarantee that the test documents will come from one of the 6 languages you have seen in the training set.\n",
    "\n",
    "The data you will use is 6 translations of part of the [Universal Declaration of Human Rights](http://www.un.org/en/universal-declaration-human-rights/index.html) (which has been translated into many languages although the data for the 6 languages is in the Language Identification folder in the Week 04 folder.)\n",
    "\n",
    "The algorithm you will use requires that you build 6 separate character bigram grammars, one for each language, on the training data. Mostly in lecture we talked about **word bigrams**. A character bigram is computed on characters instead of words. You should use the simple Bayesian Unigram Prior smoothing method.\n",
    "\n",
    "For each test document in the dev subfolder, for each of your 6 bigram grammars, you compute the log-likelihood of the test document given the bigram grammar (use the log-likelihood instead of the likelihood since it's less likely to underflow). Then you choose as your answer for that document the language that gave the highest log-likelihood.\n",
    "\n",
    "Here's the formal description of the equations you should be computing. First, you want to pick the language, out of the 6 languages, which assigns the highest log probability to the document:\n",
    "\n",
    "$$\\hat{L} = \\underset{L \\in \\mathcal{L}}{\\mathrm{argmax}} ( log P_L(Document) )$$\n",
    "\n",
    "To compute the log probability for each language, you make the Markov (N-gram) assumption, and use a bigram grammar that has been trained on that language: \n",
    "\n",
    "$$log P_L(Document) = log P_{smooth}(char^n_1) \\approx \\prod_{i=1}^n log P_{smooth}(char_i | char_{i-1})$$\n",
    "\n",
    "(That was the equation in log-space; in non-log space it would be:)\n",
    "\n",
    "$$P_L(Document) = P_{smooth}(char^n_1) \\approx \\prod_{i=1}^n P_{smooth}(char_i | char_{i-1})$$\n",
    "\n",
    "Don't forget to add some sort of special START and END characters at the beginning and end of the file.\n",
    "\n",
    "To train your bigram grammars, use Bayesian Unigram Prior smoothing:\n",
    "\n",
    "$$P_{smooth}(char_i | char_{i-1}) = \\frac{C(char_{i-1},char_i) + P(char_i)}{C(char_{i-1}) + 1}$$\n",
    "\n",
    "Please develop your solution in an iPython notebook using the text in the train subfolder. Then test your models on data in the dev subfolder.\n",
    "\n",
    "The data is in UTF-8 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
